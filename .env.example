# Maximum time one single client can wait for their request to be processed.
# Since we are batching requests for end-users, it can take a while for someone
# else to "join", and so there should be a timeout to not make first client suffer.
MAX_WAIT_TIME=10000

# Maximin client requests in a batch.
MAX_BATCH_SIZE=10

# URL of the service we are sending batch requests to
INFERENCE_SERVICE_URL=http://127.0.0.1:8080

# Optional. This is not needed just yet, since our inference service is running
# as a "sidecar" and does not require authentication (at least not just yet)
INFERENCE_SERVICE_KEY=

# Address our batching proxy application should  be listening at.
# For containerized environments, this is normally set to all availabe interfaces.
IP=127.0.0.1

# Application port.
PORT=8081

# Log filter. In production, we will want to remove the debug noise.
RUST_LOG="auto_batching_proxy=trace,axum=info"
